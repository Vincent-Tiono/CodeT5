per_device_train_batch_size: 32
per_device_eval_batch_size: 128
learning_rate: 0.001
weight_decay: 0.0
num_train_epochs: 20
max_train_steps: null
eval_train_set: true
gradient_accumulation_steps: 1
lr_scheduler_type: "linear"
num_warmup_steps: 0
warmup_ratio: 0.1
model_type: null
adafactor_for_t5: false
max_grad_norm: 1
adamw: false
mixed_precision: "fp16"
mismatch: false
mismatch_grad_norm: 0.1
mismatch_norm: 1
mismatch_lambda: 0.002
