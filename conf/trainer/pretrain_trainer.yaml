per_device_train_batch_size: 128
per_device_eval_batch_size: 128
learning_rate: 0.0005
weight_decay: 0.2
num_train_epochs: 32
max_train_steps: null
gradient_accumulation_steps: 1
lr_scheduler_type: "cosine"
num_warmup_steps: 2000
warmup_ratio: null
model_type: null
mixed_precision: "fp16"
